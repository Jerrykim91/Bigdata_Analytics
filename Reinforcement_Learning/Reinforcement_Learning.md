
# 전개 방향 

- 개별 알고리즘 -> 탐색 처리 -> 그래픽스 -> 게임AI구현 -> 게임 구현 완료


# AI

- 약 AI (현재~)
  - 하나의 업무만 잘한다
- 강 AI (2040 도래)
  - 인지능력이 사람에 수준으로 도달하는 
  - 여러개를 잘하는 AI
- 초 AI (2060 도래)
  - 인간의 인지 능력을 초월한 수준
  - 인간이 AI를 이해하지 못하는 단계


# 게임 구현 
---

## 다중 슬롯머신

### 알고리즘 정의 
--- 

- UCB1
- ε-greedy (엡실론-greedy)
- 강화학습의 핵신 아이디어 -> 탐색과 활용(Exploration & Exploitation)


### Greedy Algorithm( 그리디 알고리즘, 탐욕 알고리즘 ) 
---

- 미래를 생각하지 않고 각 단계에서 가장 최선의 선택을 하는 기법 
    - 즉, 각 단계에서 최선의 선택을 한 것이 전체적으로도 최선이길 바라는 알고리즘 

- 예 :

    - 주사위 3개를 굴린 결과가 아래와 같고, 가장 높은 숫자를 반환할 주사위를 선택한다고 가정하자.
    - 주사위 1 :  5
    - 주사위 2 :  3
    - 주사위 3 :  1
    - 주사위 4 :  6

- 그리디 알고리즘에 따르면 주사위 4를 선택 해야한다. 4개의 주사위 중에서 가장 큰 값이 나왔기 때문이다. 
- 하지만 다음에 주사위 4를 골라도 최상의 결과를 반환될 것이라는 확신을 가질 수 없다.
    - 이유는 이주사위를 한 번씩 테스트를 했기 때문이다.
    - 이말인 즉, 탐험(Exploration)이라는 행동이 충분히 이뤄지지 않았기 때문이다.  

#### **Greedy Algorithm 요약**

- **미래를 생각하지 않고 각 단계에서 가장 최선의 선택을 하는 것**
- 미래를 고려하지 않기 때문에 항상 최선의 결과를 반환 x
- 위와 같은 결과가 나왔을 때, 다음에 6이 나온 주사위를 선택한다.
- **탐험(Exploration)이 충분히 이루어지지 않았기 때문에 최상의 결과가 나올것** 이라는 확신은 x  


### E-Greedy Algorithm( 엡실론 그리디 알고리즘)
---

#### **엡실론이란 ?**

- 수학에서는 모든 양수를 가리키지만 관습적으로 0은 아니되 아주 작은 임의의 숫자를 나타낼 때 쓰인다. 결국 극한이 들어가는 수식과는 불가분한 문자. 엡실론-델타 논법이라는 유명한 논법에 쓰이기도 했다. 수학자 에르되시 팔은 아이들을 꼬마야 하는 뉘앙스로 엡실론이라 부르곤 했다고.
- 통계학에서 오차 범위를 의미한다.


- 탐험이 부족했던 Greedy Algorithm을 개선시킨 전략
- 일정한 확률로 랜덤으로 주사위를 선택하도록 하는 것
- 예:
    - 일정한 확률을 위한 *도구*로 동전을 사용한다고 할 때, 
        - 동전의 앞면이 나오면 랜덤으로 주사위를 선택 
        - 동전의 뒷면이 나오면 이전에 최선의 결과를 냈던 주사위를 선택하는 것 -> 엡실론 그리디 알고리즘 
    - 판단을 위해 사용된 동전의 앞면이 나올 확률 50%는 = Epsilon이라는 HyperParameter
    - Epsilon이라는 HyperParameter는 0 ~ 1 사이의 변수
        - 예제가 e 가 0.5에 해당되며 50%의 확률로는 주사위 6을 선택
            - 50% 나올 확률로 무작위로 주사위를 선택하게 된다. 

- 쉽게 말하면 
    - 동전을 던져 앞면이 나오면 점수 좋았던 슬롯머신, 뒷면이 나오면 랜덤으로 선택
        - 일정한 확률로 랜덤으로 슬롯머신을 선택하도록 한다. 
        - 동전의 앞면이 나올 확률은 50% 이다.  -> Epsilon이라는 HyperParameter
        - 50%의 확률을 가지고 그리디 알고리즘으로 경험상 성능이 좋았던 슬롯머신을 선택 
        - 50%의 확률로 동전 뒷면이 나오며 슬롯머신의 성능과 상관없이 랜덤으로 슬롯머신을 고른다. 

#### **E-Greedy Algorithm 요약**

- 0 ~ 1 사이 Epsilon(e)이라는 HyperParameter를 통해 행동 결정 
- 일정한 확률 e로는 Random으로 선택 
    - 1-e의 확률로는 뢰상의 결과를 냈던 행동을 선택 
- e = 0.5일때, 50% 확률로는 무작위로 주사위를 선택하고, 50% 확률로는 6이 나온 주사위를 선택한다.    

---

### 다중 슬롯머신(Slot Machine) 구현

- 스타일을 익힌다. 
- 강화 학습의 많은 요소들은 생략되어 있는 간단한 구조 
- 전체적인 절차(플로우, 흐름)을 이해할 수 있다. 


### 게임 환경 

- 슬롯머신의 팔(Arm)은 여러 개 존재할 수 있다. 
- 각 Arm을 선택할때 보상 Reward가 나올 확율은 정해졌다.

    - 1번 arm은 10%
    - 2번 arm은 50%
    - 3번 arm은 90% ...

- 게임을 시작할 때 각 팔에 대한 확률을 모른다. 


### 게임의 목적

- 제한된 횟수에서 최대의 보상을 받는다.
- 어떤 순서로 arm을 선택해야 하는가?
- 어떤 arm을 당겨야 가장 많은 돈을 벌것인가?
    - 단, 보상값들이 달라졌을 때 해당  


### 에이전트의 행동 

- 여러 슬롯 중 한 개를 선택
- 1개를 당기면 -> 행동 -> 한 턴이 종료 
- 상태가 없다. 변화가 없다. 

---

|강화 학습 요소|다중 슬롯머신 상의 내용|
|:--:|:--:|
|에이전트|슬롯 머신을 내리는 사람|
|환경|다중 슬롯 머신|
|목적|많은 보상을 획득, 제한된 횟수 내에서|
|행동|여러 개의 arm중 한개를 선택|
|에피소드|1회의 행동|
|상태|없음|
|보상|보상은 모두 동일하므로, +1.0(설정)|
|수익|보상의 총합(즉시 보상 + 지연 보상)|
|학습 방법|UCB1, ε-greedy (엡실론-greedy)|
|파라 미터 변경주기(정책결정맥락)|1회의 행동 종료 후|
|정책|하위에 별로로 기술|


---

- 슬롯머신의 Arm을 제한된 횟수내에 선택할 수 있다.
    - 10회, 100회, 1000회

- 탐색(탐험) 
    - Arm 별로 보상을 받을 확률을 모른다. 최초의 전략은 정보 수집을 하기 위해 랜덤하게 선택

- 탐색과 이용에 대한 트레이트 오프 문제가 발생 
    - 균형이 필요하다.(조화가 필요)

- 정보수집을 위해서 탐색만 수행하면, 모든 팔에 대한 보상 정보를 알게 된다.
    - 하지만 제한된 횟수라는 환경 안에서 보상이 가장 높은 팔(확률이 가장 높은 팔)만 선택하는 것보다 수익이 적게 된다.

- 반대시점, 현 시점에서 가장 높은 팔만 선택한다면, 더 많은 보상을 지급하는 팔을 선택하지 못하는 경우도 발생한다.



```py
# 구조 
'''
- SlotMachineGame
  L SlotArm Class
  L GameEngine Class
    L EpsilonGreedyEngine Class
    L UCB1Engine Class
  L GameSimulator Class or function
'''
# 필요 패키지 
import numpy as np
import pandas as pd
import random
import math

import matplotlib.pyplot as plt
%matplotlib inline

```


## 미로 게임   

### 미로 게임 구현 

- 알고리즘 
    - 정책 경사법 
    - Sarsa
    - Q Learning


## 카드 - 풀 게임 

### 카드 게임 구현 

- 강화 학습 알고리즘 
    - DQN


---


# 참고 
---
- [Multi-armed Bandit](http://sanghyukchun.github.io/96/)     
- [강화학습정리](https://brunch.co.kr/@chris-song/62)

---
