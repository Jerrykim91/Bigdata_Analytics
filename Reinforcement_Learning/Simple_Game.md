
# 전개 방향 

- 개별 알고리즘 -> 탐색 처리 -> 그래픽스 -> 게임AI구현 -> 게임 구현 완료

# AI

- 약 AI (현재~)
    - 하나의 업무만 잘한다. 
    - 범용적
- 강 AI (2040 도래) -
- 초 AI (2060 도래)

# 다중 슬롯머신 

- 알고리즘 
    - UCB1
    - ε-greedy (엡실론-greedy)


# 미로 게임   

- 알고리즘 
    - 정책 경사법 
    - Sarsa
    - Q Learning


# 카드 - 풀 게임 

- DQN

# 다중 슬롯머신(Slot Machine) 구현

- 스타일을 익힌다. 
- 강화 학습의 많은 요소들은 생략되어 있는 간단한 구조 
- 전체적인 절차(플로우, 흐름)을 이해할 수 있다. 


# 게임 환경 

- 슬롯머신의 팔(Arm)은 여러 개 존재할 수 있다. 
- 각 Arm을 선택할때 보상 Reward가 나올 확율은 정해졌다.

    - 1번 arm은 10%
    - 2번 arm은 50%
    - 3번 arm은 90% ...

- 게임을 시작할 때 각 팔에 대한 확률을 모른다. 


# 게임의 목적

- 제한된 횟수에서 최대의 보상을 받는다.
- 어떤 순서로 arm을 선택해야 하는가?
- 어떤 arm을 당겨야 가장 많은 돈을 벌것인가?
    - 단, 보상값들이 달라졌을 때 해당  


# 에이전트의 행동 

- 여러 슬롯 중 한 개를 선택
- 1개를 당기면 -> 행동 -> 한 턴이 종료 
- 상태가 없다. 변화가 없다. 

---

|강화 학습 요소|다중 슬롯머신 상의 내용|
|:--:|:--:|
|에이전트|슬롯머신을 내리는 사람|
|환경|다중 슬롯머신|
|목적|많은 보상을 획득, 제한된 횟수 내에서|
|행동|여러 개의 arm 중 한개를 선택|
|에피소드|1회의 행동|
|상태|없음|
|보상|보상은 모두 동일하므로, +1.0(설정)|
|수익|보상의 총합(즉시 보상+지연 보상)|
|학습 방법|UCB1, ε-greedy (엡실론-greedy)|
|파라 미터 변경 주기(정책 결정맥락)|1회의 행동 종료 후|
|정책||


---

- 슬롯머신의 Arm을 제한된 횟수내에 선택할 수 있다.(10회, 100회, 1000회)
- 탐색 
    - Arm 별로 보상을 받을 확률을 모른다. 최초의 전략은 정보 수집을 하기 위해 랜덤하게 선택
- 탐색과 이용에 대한 트레이트 오프 문제가 발생 
    - 균형이 필요하다.(조화가 필요)
- 정보수집을 위해서 탐색만 수행하면, 모든 팔에 대한 보상 정보를 알게 된다.
    - 하지만 제한된 횟수라는 환경 안에서 보상이 가장 높은 팔(확률이 가장 높은 팔)만 선택하는 것보다 수익이 적게 된다.
- 반대시점, 현 시점에서 가장 높은 팔만 선택한다면, 더 많은 보상을 지급하는 팔을 선택하지 못하는 경우도 발생한다.



```py
# 구조 
'''
- SlotMachineGame
  L SlotArm Class
  L GameEngine Class
    L EpsilonGreedyEngine Class
    L UCB1Engine Class
  L GameSimulator Class or function
'''
# 필요 패키지 
import numpy as np
import pandas as pd
import random
import math

import matplotlib.pyplot as plt
%matplotlib inline

```


# 알고리즘 정의 

## Greedy Algorithm(그리디 알고리즘, 탐욕 알고리즘) 


## E-Greedy Algorithm

